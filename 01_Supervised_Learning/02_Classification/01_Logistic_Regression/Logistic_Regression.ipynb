{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this code, we will explore the concept of [*Logistic Regression*](https://en.wikipedia.org/wiki/Logistic_regression) and its application for sentimental analysis. \n",
    "\n",
    "The goal is to use the [amazon_baby_subset.csv](../Data/amazon_baby_subset.csv), which contains 4 columns: Product name, client review, client rate, and sentiment. The rating goes from 1 (worst) to 5 (best) and the sentiment is -1 if the rating is low (< 3) and 1 if it is good (>= 3). Here, the logistic regression method is used to give weights to each important word in the comments (the important words are given now. In the future, we will see how to select them) and to create a prediction model for future reviews, understanting if it is good or bad.\n",
    "\n",
    "First, let's load all used packages and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mguarido/anaconda2/envs/venv/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import turicreate as tc\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /mnt/hgfs/OneDrive/Machine_Learning/01_Supervised_Learning/02_Classification/Data/amazon_baby_subset.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /mnt/hgfs/OneDrive/Machine_Learning/01_Supervised_Learning/02_Classification/Data/amazon_baby_subset.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.0377 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.0377 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,int,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /mnt/hgfs/OneDrive/Machine_Learning/01_Supervised_Learning/02_Classification/Data/amazon_baby_subset.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /mnt/hgfs/OneDrive/Machine_Learning/01_Supervised_Learning/02_Classification/Data/amazon_baby_subset.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 53072 lines in 0.885587 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 53072 lines in 0.885587 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "products = tc.SFrame('../Data/amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">rating</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All of my kids have cried<br>non-stop when I tried to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Nature's Lullabies Second<br>Year Sticker Calendar ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">We wanted to get<br>something to keep track ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Nature's Lullabies Second<br>Year Sticker Calendar ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">My daughter had her 1st<br>baby over a year ago. ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Lamaze Peekaboo, I Love<br>You ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">One of baby's first and<br>favorite books, and i ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">SoftPlay Peek-A-Boo<br>Where's Elmo A Childr ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Very cute interactive<br>book! My son loves this ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Our Baby Girl Memory Book</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Beautiful book, I love it<br>to record cherished t ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Hunnt&amp;reg; Falling<br>Flowers and Birds Kids ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Try this out for a spring<br>project !Easy ,fun and ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Blessed By Pope Benedict<br>XVI Divine Mercy Full ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">very nice Divine Mercy<br>Pendant of Jesus now on ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Cloth Diaper Pins<br>Stainless Steel ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">We bought the pins as my<br>6 year old Autistic son ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Cloth Diaper Pins<br>Stainless Steel ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">It has been many years<br>since we needed diaper ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tname\tstr\n",
       "\treview\tstr\n",
       "\trating\tint\n",
       "\tsentiment\tint\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "|              name             |             review            | rating | sentiment |\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "| Stop Pacifier Sucking with... | All of my kids have cried ... |   5    |     1     |\n",
       "| Nature's Lullabies Second ... | We wanted to get something... |   5    |     1     |\n",
       "| Nature's Lullabies Second ... | My daughter had her 1st ba... |   5    |     1     |\n",
       "|  Lamaze Peekaboo, I Love You  | One of baby's first and fa... |   4    |     1     |\n",
       "| SoftPlay Peek-A-Boo Where'... | Very cute interactive book... |   5    |     1     |\n",
       "|   Our Baby Girl Memory Book   | Beautiful book, I love it ... |   5    |     1     |\n",
       "| Hunnt&reg; Falling Flowers... | Try this out for a spring ... |   5    |     1     |\n",
       "| Blessed By Pope Benedict X... | very nice Divine Mercy Pen... |   5    |     1     |\n",
       "| Cloth Diaper Pins Stainles... | We bought the pins as my 6... |   4    |     1     |\n",
       "| Cloth Diaper Pins Stainles... | It has been many years sin... |   5    |     1     |\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count how many positive and negative reviews the data set has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews = 26579\n",
      "Number of negative reviews = 26493\n"
     ]
    }
   ],
   "source": [
    "print 'Number of positive reviews =', len(products[products['sentiment']==1])\n",
    "print 'Number of negative reviews =', len(products[products['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the reviews are writen have punctuation. Let's clean them and also create columns with the important words count. The important words are in the file [important_words.json](../Data/important_words.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the important words from the *json* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'one', 'great', 'love', 'use', 'would', 'like', 'easy', 'little', 'seat', 'old', 'well', 'get', 'also', 'really', 'son', 'time', 'bought', 'product', 'good', 'daughter', 'much', 'loves', 'stroller', 'put', 'months', 'car', 'still', 'back', 'used', 'recommend', 'first', 'even', 'perfect', 'nice', 'bag', 'two', 'using', 'got', 'fit', 'around', 'diaper', 'enough', 'month', 'price', 'go', 'could', 'soft', 'since', 'buy', 'room', 'works', 'made', 'child', 'keep', 'size', 'small', 'need', 'year', 'big', 'make', 'take', 'easily', 'think', 'crib', 'clean', 'way', 'quality', 'thing', 'better', 'without', 'set', 'new', 'every', 'cute', 'best', 'bottles', 'work', 'purchased', 'right', 'lot', 'side', 'happy', 'comfortable', 'toy', 'able', 'kids', 'bit', 'night', 'long', 'fits', 'see', 'us', 'another', 'play', 'day', 'money', 'monitor', 'tried', 'thought', 'never', 'item', 'hard', 'plastic', 'however', 'disappointed', 'reviews', 'something', 'going', 'pump', 'bottle', 'cup', 'waste', 'return', 'amazon', 'different', 'top', 'want', 'problem', 'know', 'water', 'try', 'received', 'sure', 'times', 'chair', 'find', 'hold', 'gate', 'open', 'bottom', 'away', 'actually', 'cheap', 'worked', 'getting', 'ordered', 'came', 'milk', 'bad', 'part', 'worth', 'found', 'cover', 'many', 'design', 'looking', 'weeks', 'say', 'wanted', 'look', 'place', 'purchase', 'looks', 'second', 'piece', 'box', 'pretty', 'trying', 'difficult', 'together', 'though', 'give', 'started', 'anything', 'last', 'company', 'come', 'returned', 'maybe', 'took', 'broke', 'makes', 'stay', 'instead', 'idea', 'head', 'said', 'less', 'went', 'working', 'high', 'unit', 'seems', 'picture', 'completely', 'wish', 'buying', 'babies', 'won', 'tub', 'almost', 'either']\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/important_words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]\n",
    "print important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the reviews punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to count each important word and create new columns with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 53072\n",
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, ... ]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[important_words[0]] # This is the count of the word 'baby' for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12174"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products[products[important_words[0]] > 0]) # Number of reviews that contain the word 'baby'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many time the word *baby* appears in all the reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(products['baby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression deals with categorical variables, and here in the sentimental analysis, it assumes only 2 number: 1 for a good review and -1 for a bad review.\n",
    "\n",
    "The whole idea is to estimate the probabily on which the review is good or bad, and the probability is computed usin the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) (also known as link function):\n",
    "\n",
    "$$ P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + e^{-\\mathbf{w}^T h(\\mathbf{x}_i)}} $$\n",
    "\n",
    "where, in our code, the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review $\\mathbf{x}_i$. In this equation, for a given vector of weights $\\mathbf{w}$ and the word count $\\mathbf{x}_i$, it computes the probabilty of the sentiment $\\mathbf{i}_i$ be positive (equal to 1). The values go from 0 to 1. We can choose a threshold (usually 0.5) on which a higher probability is considered as a positive sentiment, while the opsite comes from a lower probability.\n",
    "\n",
    "Best prediction comes from optimized weights. And a good way to obtain such weights is by [maximizing the likelihood function](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). The [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) is writen as:\n",
    "\n",
    "$$\\ell(\\mathbf{w}) = \\prod_{i = 1}^{N}P(y_i | \\mathbf{x}_i,\\mathbf{w})$$\n",
    "\n",
    "The optimization method can be done by by the gradient descent method. To find the optimized weights, we should derive the likelihood function by $\\mathbf{w}$. However, such task is not easy for the current function. A good strategy is to work with the [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm) of the likelihood function, called the **log-likelihood**.\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\ln\\prod_{i = 1}^{N}P(y_i | \\mathbf{x}_i,\\mathbf{w}) = \\sum_{i = 1}^{N}\\ln P(y_i | \\mathbf{x}_i,\\mathbf{w})$$\n",
    "\n",
    "The log-likelihood can be computed using the following formula:\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\{ (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln[1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))]\\} $$\n",
    "\n",
    "The derivative of the log-likelihood is:\n",
    "\n",
    "$$\\frac{\\partial\\ell\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)$$\n",
    "\n",
    "where $\\mathbf{1}[y_i = +1]$ means that it is equal 1 if the sentment is positive (1) and 0 if the sentiment is negative (-1). Now that we can compute the gradient of the log-likelihood, the gradient descent method can be applied:\n",
    "\n",
    "$$\\mathbf{w}_{j+1} = \\mathbf{w}_j + \\eta\\frac{\\partial\\ell\\ell}{\\partial w_j}$$\n",
    "\n",
    "where $\\eta$ is the step length.\n",
    "\n",
    "Great!!! Now let's implement the equations above to find the optimal weights $\\mathbf{w}$. Then we can use then to make predictions of the reviews sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the Turicreate SFrame data to Numpy array to perform the math. The following function receives the SFrame data, the list of desired features (that will be the important words count), and the class label (in this case, 'sentiment'). It will return two outputs: a matrix with the features (word count) plus an initial intercept (= 1), and an array with the sentiment (-1 or +1) of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1 # Initial value for the intercept, including it in the sframe\n",
    "    features = ['intercept'] + features # Including 'intercept' in the features array\n",
    "    features_sframe = data_sframe[features] # Saving a sframe with only the desired features \n",
    "    feature_matrix = features_sframe.to_numpy() # Converting the features sframe to a numpy array (matrix)\n",
    "    label_sarray = data_sframe[label] # Picking the desired label\n",
    "    label_array = label_sarray.to_numpy() # Converting the label to a numpy array\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, sentiment = get_numpy_data(products, important_words, 'sentiment')\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53072\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "print len(products[products['sentiment']]) # Number of reviews\n",
    "print len(important_words) + 1 # Number of features plus the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the *feature_matrix* matches in size the number of reviews and the number of features (plus the intercept).\n",
    "\n",
    "Let's now create a function to calculate the probability for positive sentiments (the negative sentiment probability is just one minus the positive sentiment probability), given an array of coefficients and the feature matrix. The output is an array with the probability predictions for each reviews (or length of the feature matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(coefficients, feature_matrix):\n",
    "    # Computing P(y_i = +1 | x_i, w), using the logistic function\n",
    "    predictions = 1/(1 + np.exp(-np.dot(coefficients, feature_matrix)))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "\n",
    "* **errors** vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* **feature** vector containing $h_j(\\mathbf{x}_i)$  for all $i$. \n",
    "\n",
    "The derivative is just the dot product of the errors and the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    derivative = np.dot(errors, feature)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a function to compute the log-likelihood for all the features. It is interesting to use it as a QC tool. The log-likelihood, in the gradient descent method, should increase at each interation, until a maximum is reached.\n",
    "\n",
    "The function has three inputs: the features matris, the sentiment array, and the computed (or initial) coefficients. The output is a scalar with the value of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    isone = (sentiment==+1) # Saving an array with positive sentiments as 1 and others as 0\n",
    "    \n",
    "    # Computing each part of the log-likelihood formula\n",
    "    dotfc = np.dot(feature_matrix, coefficients)\n",
    "    lnexp = np.log(1. + np.exp(-dotfc))\n",
    "    \n",
    "    # Avoiding infinite results.\n",
    "    mask = np.isinf(lnexp)\n",
    "    lnexp[mask] = -dotfc[mask]\n",
    "    \n",
    "    # log-likelihood\n",
    "    ll = np.sum((isone-1)*dotfc - lnexp)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the three functions above to do the logistic regression using the gradient descent method.\n",
    "The next function receives the feature matrix, the sentiment array, the initial guess for the coefficients, the step length, and the maximum number of iterations. The output are the coefficients for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    \n",
    "    # Start the gradient method and stop it in the maximum iteration\n",
    "    for itr in xrange(max_iter):\n",
    "\n",
    "        # Making the predictions with the initial or updated coefficients coefficients\n",
    "        predictions = predict_probability(coefficients, np.transpose(feature_matrix))\n",
    "        \n",
    "        # Compute indicator value for +1 (positive sentiment)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors with the initial or updated coefficients coefficients\n",
    "        errors = indicator - predictions\n",
    "        \n",
    "        # Apply the gradient method for each coefficient\n",
    "        for j in xrange(len(coefficients)):\n",
    "            \n",
    "            # Update the coefficient for feature j\n",
    "            coefficients[j] = coefficients[j] + step_size * np.sum(feature_derivative(errors, feature_matrix[:,j]))\n",
    "            \n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'Iteration %*d: log-likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0: log-likelihood of observed labels = -36780.91768478\n",
      "Iteration   1: log-likelihood of observed labels = -36775.13434712\n",
      "Iteration   2: log-likelihood of observed labels = -36769.35713564\n",
      "Iteration   3: log-likelihood of observed labels = -36763.58603240\n",
      "Iteration   4: log-likelihood of observed labels = -36757.82101962\n",
      "Iteration   5: log-likelihood of observed labels = -36752.06207964\n",
      "Iteration   6: log-likelihood of observed labels = -36746.30919497\n",
      "Iteration   7: log-likelihood of observed labels = -36740.56234821\n",
      "Iteration   8: log-likelihood of observed labels = -36734.82152213\n",
      "Iteration   9: log-likelihood of observed labels = -36729.08669961\n",
      "Iteration  10: log-likelihood of observed labels = -36723.35786366\n",
      "Iteration  11: log-likelihood of observed labels = -36717.63499744\n",
      "Iteration  12: log-likelihood of observed labels = -36711.91808422\n",
      "Iteration  13: log-likelihood of observed labels = -36706.20710739\n",
      "Iteration  14: log-likelihood of observed labels = -36700.50205049\n",
      "Iteration  15: log-likelihood of observed labels = -36694.80289716\n",
      "Iteration  20: log-likelihood of observed labels = -36666.39512033\n",
      "Iteration  30: log-likelihood of observed labels = -36610.01327118\n",
      "Iteration  40: log-likelihood of observed labels = -36554.19728365\n",
      "Iteration  50: log-likelihood of observed labels = -36498.93316099\n",
      "Iteration  60: log-likelihood of observed labels = -36444.20783914\n",
      "Iteration  70: log-likelihood of observed labels = -36390.00909449\n",
      "Iteration  80: log-likelihood of observed labels = -36336.32546144\n",
      "Iteration  90: log-likelihood of observed labels = -36283.14615871\n",
      "Iteration 100: log-likelihood of observed labels = -36230.46102347\n",
      "Iteration 200: log-likelihood of observed labels = -35728.89418769\n",
      "Iteration 300: log-likelihood of observed labels = -35268.51212683\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients=np.zeros(194),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the coefficients in hands, let's predict the sentiments.\n",
    "\n",
    "For this analysis, I am chosing to classify probabilities larger than 0.5 as positive and from 0.5 to 0 as negative. So:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) > 0.5 \\\\\n",
    "      -1 & P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) \\leq 0.5 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Time to compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_probability(feature_matrix,coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_predictions = tc.SArray(pred).apply(lambda x: 1 if x > 0.5 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25126"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classify_predictions[classify_predictions == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26579"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment[sentiment == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction is close to the true sentiment (apparently). But we don't know if the -1's and 1's are on the correct reviews. So, let's compare the predictions with the true reviews and compute the **accuracy** of our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct reviews: 39903\n",
      "Number of wrong reviews: 13169\n",
      "Number of reviews: 53072\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "num_total = len(products) # Total number of reviews\n",
    "num_correct = (classify_predictions == products['sentiment']).sum() # Is equal, return 1 (TRUE). Else, return 0 (FALSE)\n",
    "num_wrong = num_total - num_correct\n",
    "accuracy = 1.0*num_correct/num_total\n",
    "\n",
    "print 'Number of correct reviews:', num_correct\n",
    "print 'Number of wrong reviews:', num_wrong\n",
    "print 'Number of reviews:', num_total\n",
    "print 'Accuracy: %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our result with the built in logistic classifier of Turicreate;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 50480</pre>"
      ],
      "text/plain": [
       "Number of examples          : 50480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 193</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 193</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 194</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-accuracy | Validation-accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-accuracy | Validation-accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 4.432989     | 0.791086          | 0.798611            |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 4.432989     | 0.791086          | 0.798611            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 6.188983     | 0.791284          | 0.799383            |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 6.188983     | 0.791284          | 0.799383            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 7.877486     | 0.791363          | 0.799769            |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 7.877486     | 0.791363          | 0.799769            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 9.610612     | 0.791521          | 0.798997            |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 9.610612     | 0.791521          | 0.798997            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 6        | 11.335757    | 0.791482          | 0.798611            |</pre>"
      ],
      "text/plain": [
       "| 5         | 6        | 11.335757    | 0.791482          | 0.798611            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 7        | 13.038990    | 0.791482          | 0.798611            |</pre>"
      ],
      "text/plain": [
       "| 6         | 7        | 13.038990    | 0.791482          | 0.798611            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modeltc = tc.logistic_classifier.create(products, target='sentiment', features = important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our code: 0.75\n",
      "Accuracy of Turicreate: 0.79\n",
      "[1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n",
      "[1, -1, 1, 1, 1, 1, 1, 1, 1, -1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "predictionstc = modeltc.predict(products, output_type = 'class')\n",
    "resultstc = modeltc.evaluate(products)\n",
    "print 'Accuracy of our code: %.2f' % accuracy\n",
    "print 'Accuracy of Turicreate: %.2f' % resultstc['accuracy']\n",
    "print classify_predictions.head()\n",
    "print predictionstc.head()\n",
    "print products.head()['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got pretty close to the Turicreate package. We can improve the accuracy by penalizing some words, as we did with the [polynomial regression](../Polynomial_Regression/Polynomial_Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with L2 penalization (regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as in the [polynomial regression](../Polynomial_Regression/Polynomial_Regression.ipynb), we will include a term in the cost function to penalize large coefficients (overfitting). For the \"simple\" logistic regression, the cost function is the log likelihood:\n",
    "\n",
    "$$Cost(\\mathbf{w}) = \\ell\\ell(\\mathbf{w})$$\n",
    "\n",
    "Now, we include the penalizations term: the L2-norm of the coefficients multiplied by the tuning parameter $\\lambda$:\n",
    "\n",
    "$$Cost(\\mathbf{w}) = \\ell\\ell(\\mathbf{w}) - \\lambda \\|\\mathbf{w}\\|_{2}^{2}$$\n",
    "\n",
    "To find the best coefficients, we have to take the derivative of the cost function. The derivative of the cost function is known. The derivative of the penalty term is, for the j-th coefficient is:\n",
    "\n",
    "$$\\frac{\\partial \\|\\mathbf{w}\\|_{2}^{2}}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}(w_0^2 + w_1^2 + w_2^2 + \\dots + w_j^2 + \\dots + w_N^2) = 2w_j$$\n",
    "\n",
    "For the gradient descent, the update for the coefficients will be:\n",
    "\n",
    "$$\\mathbf{w}_{j+1} = \\mathbf{w}_j + \\eta \\big( \\frac{\\partial\\ell\\ell}{\\partial w_j} - 2w_j \\big)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\frac{\\partial\\ell\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)$$\n",
    "\n",
    "Usually the intercept does not suffer any kind of penalty. So, when we create our functions, let's keep it in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the data into the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   : 42474 data points\n",
      "Validation set : 10598 data points\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data = products.random_split(.8, seed = 1)\n",
    "\n",
    "print 'Training set   : %d data points' % len(train_data)\n",
    "print 'Validation set : %d data points' % len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert both sets to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient now must include the $-2w_j$ for all the coefficients but the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, intercept): \n",
    "    derivative = np.dot(errors, feature)\n",
    "\n",
    "    # Add the L2 penalty to all coefficients but the intercept\n",
    "    if not intercept:\n",
    "        derivative -= 2 * l2_penalty * coefficient\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function (log-likelihood) also need the L2 term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    isone = (sentiment==+1)\n",
    "    lnexp = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((isone-1)*lnexp - np.log(1. + np.exp(-lnexp))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "\n",
    "    # Start the gradient method and stop it in the maximum iteration\n",
    "    for itr in xrange(max_iter):\n",
    "\n",
    "        # Making the predictions with the initial or updated coefficients coefficients\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for +1 (positive sentiment)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors with the initial or updated coefficients coefficients\n",
    "        errors = indicator - predictions\n",
    "\n",
    "        # Apply the gradient method for each coefficient\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "\n",
    "            # Computing the derivative\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n",
    "            \n",
    "            # Updating the coefficient\n",
    "            coefficients[j] += step_size * derivative\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check different values for the l2_penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29256.53509469\n",
      "iteration   1: log likelihood of observed labels = -29080.15933459\n",
      "iteration   2: log likelihood of observed labels = -28910.70310738\n",
      "iteration   3: log likelihood of observed labels = -28747.51727534\n",
      "iteration   4: log likelihood of observed labels = -28590.11088099\n",
      "iteration   5: log likelihood of observed labels = -28438.09355101\n",
      "iteration   6: log likelihood of observed labels = -28291.14073932\n",
      "iteration   7: log likelihood of observed labels = -28148.97259118\n",
      "iteration   8: log likelihood of observed labels = -28011.34094170\n",
      "iteration   9: log likelihood of observed labels = -27878.02119960\n",
      "iteration  10: log likelihood of observed labels = -27748.80718346\n",
      "iteration  11: log likelihood of observed labels = -27623.50775262\n",
      "iteration  12: log likelihood of observed labels = -27501.94453552\n",
      "iteration  13: log likelihood of observed labels = -27383.95033346\n",
      "iteration  14: log likelihood of observed labels = -27269.36794470\n",
      "iteration  15: log likelihood of observed labels = -27158.04925452\n",
      "iteration  20: log likelihood of observed labels = -26645.77896996\n",
      "iteration  30: log likelihood of observed labels = -25800.46772325\n",
      "iteration  40: log likelihood of observed labels = -25130.10730174\n",
      "iteration  50: log likelihood of observed labels = -24583.84195218\n",
      "iteration  60: log likelihood of observed labels = -24128.88105274\n",
      "iteration  70: log likelihood of observed labels = -23743.17973900\n",
      "iteration  80: log likelihood of observed labels = -23411.37372055\n",
      "iteration  90: log likelihood of observed labels = -23122.41785524\n",
      "iteration 100: log likelihood of observed labels = -22868.15757448\n",
      "iteration 200: log likelihood of observed labels = -21356.13186359\n",
      "iteration 300: log likelihood of observed labels = -20643.31510369\n",
      "iteration 400: log likelihood of observed labels = -20223.85995318\n",
      "iteration 500: log likelihood of observed labels = -19947.44361148\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 0\n",
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=0, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29256.53882048\n",
      "iteration   1: log likelihood of observed labels = -29080.18103708\n",
      "iteration   2: log likelihood of observed labels = -28910.75588332\n",
      "iteration   3: log likelihood of observed labels = -28747.61324430\n",
      "iteration   4: log likelihood of observed labels = -28590.26130285\n",
      "iteration   5: log likelihood of observed labels = -28438.30891072\n",
      "iteration   6: log likelihood of observed labels = -28291.43081329\n",
      "iteration   7: log likelihood of observed labels = -28149.34650266\n",
      "iteration   8: log likelihood of observed labels = -28011.80720946\n",
      "iteration   9: log likelihood of observed labels = -27878.58778173\n",
      "iteration  10: log likelihood of observed labels = -27749.48151749\n",
      "iteration  11: log likelihood of observed labels = -27624.29679256\n",
      "iteration  12: log likelihood of observed labels = -27502.85478602\n",
      "iteration  13: log likelihood of observed labels = -27384.98788148\n",
      "iteration  14: log likelihood of observed labels = -27270.53848873\n",
      "iteration  15: log likelihood of observed labels = -27159.35813161\n",
      "iteration  20: log likelihood of observed labels = -26647.84855939\n",
      "iteration  30: log likelihood of observed labels = -25804.30859388\n",
      "iteration  40: log likelihood of observed labels = -25135.91962912\n",
      "iteration  50: log likelihood of observed labels = -24591.73097195\n",
      "iteration  60: log likelihood of observed labels = -24138.89770310\n",
      "iteration  70: log likelihood of observed labels = -23755.34248532\n",
      "iteration  80: log likelihood of observed labels = -23425.68096021\n",
      "iteration  90: log likelihood of observed labels = -23138.85530857\n",
      "iteration 100: log likelihood of observed labels = -22886.70286101\n",
      "iteration 200: log likelihood of observed labels = -21393.98632377\n",
      "iteration 300: log likelihood of observed labels = -20697.33863201\n",
      "iteration 400: log likelihood of observed labels = -20291.58738684\n",
      "iteration 500: log likelihood of observed labels = -20026.97325778\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 4\n",
    "coefficients_4_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=4, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29256.53602614\n",
      "iteration   1: log likelihood of observed labels = -29080.16476032\n",
      "iteration   2: log likelihood of observed labels = -28910.71630190\n",
      "iteration   3: log likelihood of observed labels = -28747.54126902\n",
      "iteration   4: log likelihood of observed labels = -28590.14848947\n",
      "iteration   5: log likelihood of observed labels = -28438.14739631\n",
      "iteration   6: log likelihood of observed labels = -28291.21326647\n",
      "iteration   7: log likelihood of observed labels = -28149.06608203\n",
      "iteration   8: log likelihood of observed labels = -28011.45752709\n",
      "iteration   9: log likelihood of observed labels = -27878.16287030\n",
      "iteration  10: log likelihood of observed labels = -27748.97580016\n",
      "iteration  11: log likelihood of observed labels = -27623.70505522\n",
      "iteration  12: log likelihood of observed labels = -27502.17215166\n",
      "iteration  13: log likelihood of observed labels = -27384.20978639\n",
      "iteration  14: log likelihood of observed labels = -27269.66066064\n",
      "iteration  15: log likelihood of observed labels = -27158.37656936\n",
      "iteration  20: log likelihood of observed labels = -26646.29656688\n",
      "iteration  30: log likelihood of observed labels = -25801.42848793\n",
      "iteration  40: log likelihood of observed labels = -25131.56147426\n",
      "iteration  50: log likelihood of observed labels = -24585.81604009\n",
      "iteration  60: log likelihood of observed labels = -24131.38798639\n",
      "iteration  70: log likelihood of observed labels = -23746.22432529\n",
      "iteration  80: log likelihood of observed labels = -23414.95574317\n",
      "iteration  90: log likelihood of observed labels = -23126.53392202\n",
      "iteration 100: log likelihood of observed labels = -22872.80226137\n",
      "iteration 200: log likelihood of observed labels = -21365.62854260\n",
      "iteration 300: log likelihood of observed labels = -20656.89011412\n",
      "iteration 400: log likelihood of observed labels = -20240.90500541\n",
      "iteration 500: log likelihood of observed labels = -19967.48904706\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=1, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29256.62823956\n",
      "iteration   1: log likelihood of observed labels = -29080.70154526\n",
      "iteration   2: log likelihood of observed labels = -28912.02080432\n",
      "iteration   3: log likelihood of observed labels = -28749.91187801\n",
      "iteration   4: log likelihood of observed labels = -28593.86180762\n",
      "iteration   5: log likelihood of observed labels = -28443.46039146\n",
      "iteration   6: log likelihood of observed labels = -28298.36496072\n",
      "iteration   7: log likelihood of observed labels = -28158.27896711\n",
      "iteration   8: log likelihood of observed labels = -28022.93880567\n",
      "iteration   9: log likelihood of observed labels = -27892.10557528\n",
      "iteration  10: log likelihood of observed labels = -27765.55981884\n",
      "iteration  11: log likelihood of observed labels = -27643.09807298\n",
      "iteration  12: log likelihood of observed labels = -27524.53052351\n",
      "iteration  13: log likelihood of observed labels = -27409.67934231\n",
      "iteration  14: log likelihood of observed labels = -27298.37744939\n",
      "iteration  15: log likelihood of observed labels = -27190.46754507\n",
      "iteration  20: log likelihood of observed labels = -26696.88617495\n",
      "iteration  30: log likelihood of observed labels = -25894.76398701\n",
      "iteration  40: log likelihood of observed labels = -25271.99174237\n",
      "iteration  50: log likelihood of observed labels = -24775.34147089\n",
      "iteration  60: log likelihood of observed labels = -24370.68306980\n",
      "iteration  70: log likelihood of observed labels = -24035.18485190\n",
      "iteration  80: log likelihood of observed labels = -23753.00856326\n",
      "iteration  90: log likelihood of observed labels = -23512.82055988\n",
      "iteration 100: log likelihood of observed labels = -23306.29184986\n",
      "iteration 200: log likelihood of observed labels = -22206.54801478\n",
      "iteration 300: log likelihood of observed labels = -21802.69848642\n",
      "iteration 400: log likelihood of observed labels = -21618.08474460\n",
      "iteration 500: log likelihood of observed labels = -21523.73500491\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 1e2\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=1e2, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29257.46654341\n",
      "iteration   1: log likelihood of observed labels = -29085.54856449\n",
      "iteration   2: log likelihood of observed labels = -28923.72175265\n",
      "iteration   3: log likelihood of observed labels = -28771.03546664\n",
      "iteration   4: log likelihood of observed labels = -28626.73397364\n",
      "iteration   5: log likelihood of observed labels = -28490.18963521\n",
      "iteration   6: log likelihood of observed labels = -28360.86278202\n",
      "iteration   7: log likelihood of observed labels = -28238.27724819\n",
      "iteration   8: log likelihood of observed labels = -28122.00512090\n",
      "iteration   9: log likelihood of observed labels = -28011.65695208\n",
      "iteration  10: log likelihood of observed labels = -27906.87523962\n",
      "iteration  11: log likelihood of observed labels = -27807.32988968\n",
      "iteration  12: log likelihood of observed labels = -27712.71489976\n",
      "iteration  13: log likelihood of observed labels = -27622.74581080\n",
      "iteration  14: log likelihood of observed labels = -27537.15765924\n",
      "iteration  15: log likelihood of observed labels = -27455.70326690\n",
      "iteration  20: log likelihood of observed labels = -27102.85976864\n",
      "iteration  30: log likelihood of observed labels = -26602.70681872\n",
      "iteration  40: log likelihood of observed labels = -26281.84513484\n",
      "iteration  50: log likelihood of observed labels = -26071.02818801\n",
      "iteration  60: log likelihood of observed labels = -25929.86370532\n",
      "iteration  70: log likelihood of observed labels = -25833.87483908\n",
      "iteration  80: log likelihood of observed labels = -25767.76943721\n",
      "iteration  90: log likelihood of observed labels = -25721.75427160\n",
      "iteration 100: log likelihood of observed labels = -25689.42901741\n",
      "iteration 200: log likelihood of observed labels = -25610.71994944\n",
      "iteration 300: log likelihood of observed labels = -25607.07628120\n",
      "iteration 400: log likelihood of observed labels = -25606.85329643\n",
      "iteration 500: log likelihood of observed labels = -25606.83738060\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 1e3\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=1e3, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29349.67996719\n",
      "iteration   1: log likelihood of observed labels = -29349.56226752\n",
      "iteration   2: log likelihood of observed labels = -29349.55642623\n",
      "iteration   3: log likelihood of observed labels = -29349.55401770\n",
      "iteration   4: log likelihood of observed labels = -29349.55195595\n",
      "iteration   5: log likelihood of observed labels = -29349.55009433\n",
      "iteration   6: log likelihood of observed labels = -29349.54840914\n",
      "iteration   7: log likelihood of observed labels = -29349.54688349\n",
      "iteration   8: log likelihood of observed labels = -29349.54550226\n",
      "iteration   9: log likelihood of observed labels = -29349.54425177\n",
      "iteration  10: log likelihood of observed labels = -29349.54311966\n",
      "iteration  11: log likelihood of observed labels = -29349.54209472\n",
      "iteration  12: log likelihood of observed labels = -29349.54116680\n",
      "iteration  13: log likelihood of observed labels = -29349.54032672\n",
      "iteration  14: log likelihood of observed labels = -29349.53956616\n",
      "iteration  15: log likelihood of observed labels = -29349.53887760\n",
      "iteration  20: log likelihood of observed labels = -29349.53629754\n",
      "iteration  30: log likelihood of observed labels = -29349.53377390\n",
      "iteration  40: log likelihood of observed labels = -29349.53284036\n",
      "iteration  50: log likelihood of observed labels = -29349.53249503\n",
      "iteration  60: log likelihood of observed labels = -29349.53236728\n",
      "iteration  70: log likelihood of observed labels = -29349.53232002\n",
      "iteration  80: log likelihood of observed labels = -29349.53230254\n",
      "iteration  90: log likelihood of observed labels = -29349.53229608\n",
      "iteration 100: log likelihood of observed labels = -29349.53229368\n",
      "iteration 200: log likelihood of observed labels = -29349.53229228\n",
      "iteration 300: log likelihood of observed labels = -29349.53229228\n",
      "iteration 400: log likelihood of observed labels = -29349.53229228\n",
      "iteration 500: log likelihood of observed labels = -29349.53229228\n"
     ]
    }
   ],
   "source": [
    "# L2_penalty = 1e5\n",
    "coefficients_1e5_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=1e5, max_iter=501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the coefficients for each l2-penalty. We are going to create a table that will merge all the coefficients for all the features plus the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tc.SFrame({'word': ['(intercept)'] + important_words})\n",
    "def add_coefficients_to_table(coefficients, column_name):\n",
    "    table[column_name] = coefficients\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=0]</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=4]</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=10]</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=1e2]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">(intercept)</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0745874045386</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0739703019154</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0744325272678</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0609039624246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">baby</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0914044653289</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0912103099182</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0913556716001</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0872120460521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">one</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.022029160162</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0217417002678</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0219568925235</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0159507965338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">great</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.80062465855</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.795865100799</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.799428111181</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.699758211124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">love</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.03975150459</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.03225253965</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0378658425</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.88209210377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">use</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.012980586266</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0131784070551</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0130303425931</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0170725614152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">would</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.290384002777</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.289359116939</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.290126305589</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.268704669272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">like</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00915773854399</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00918981709201</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00916578711491</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00985497846215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">easy</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.974789783574</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.967963138841</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.973073202649</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.831173338471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">little</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.527735163007</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.524703617655</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.526972975907</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.463614351574</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=1e3]</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">coefficients [L2=1e5]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00949455273584</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00235878386375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0665918463126</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00192984031292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00184159157178</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00156175667368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.373207112946</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00880804781406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.415568149057</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00906466298034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0233987111383</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.000509449689132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.189556697274</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.00814952117861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0102424153145</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.000945362361057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.400522104686</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0087949757352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.254306309532</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00602985393423</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[194 rows x 7 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tword\tstr\n",
       "\tcoefficients [L2=0]\tfloat\n",
       "\tcoefficients [L2=4]\tfloat\n",
       "\tcoefficients [L2=10]\tfloat\n",
       "\tcoefficients [L2=1e2]\tfloat\n",
       "\tcoefficients [L2=1e3]\tfloat\n",
       "\tcoefficients [L2=1e5]\tfloat\n",
       "\n",
       "Rows: 194\n",
       "\n",
       "Data:\n",
       "+-------------+---------------------+---------------------+----------------------+\n",
       "|     word    | coefficients [L2=0] | coefficients [L2=4] | coefficients [L2=10] |\n",
       "+-------------+---------------------+---------------------+----------------------+\n",
       "| (intercept) |   -0.0745874045386  |   -0.0739703019154  |   -0.0744325272678   |\n",
       "|     baby    |   0.0914044653289   |   0.0912103099182   |   0.0913556716001    |\n",
       "|     one     |    0.022029160162   |   0.0217417002678   |   0.0219568925235    |\n",
       "|    great    |    0.80062465855    |    0.795865100799   |    0.799428111181    |\n",
       "|     love    |    1.03975150459    |    1.03225253965    |     1.0378658425     |\n",
       "|     use     |    0.012980586266   |   0.0131784070551   |   0.0130303425931    |\n",
       "|    would    |   -0.290384002777   |   -0.289359116939   |   -0.290126305589    |\n",
       "|     like    |  -0.00915773854399  |  -0.00918981709201  |  -0.00916578711491   |\n",
       "|     easy    |    0.974789783574   |    0.967963138841   |    0.973073202649    |\n",
       "|    little   |    0.527735163007   |    0.524703617655   |    0.526972975907    |\n",
       "+-------------+---------------------+---------------------+----------------------+\n",
       "+-----------------------+-----------------------+-----------------------+\n",
       "| coefficients [L2=1e2] | coefficients [L2=1e3] | coefficients [L2=1e5] |\n",
       "+-----------------------+-----------------------+-----------------------+\n",
       "|    -0.0609039624246   |   -0.00949455273584   |    0.00235878386375   |\n",
       "|    0.0872120460521    |    0.0665918463126    |    0.00192984031292   |\n",
       "|    0.0159507965338    |   -0.00184159157178   |   -0.00156175667368   |\n",
       "|     0.699758211124    |     0.373207112946    |    0.00880804781406   |\n",
       "|     0.88209210377     |     0.415568149057    |    0.00906466298034   |\n",
       "|    0.0170725614152    |    0.0233987111383    |   0.000509449689132   |\n",
       "|    -0.268704669272    |    -0.189556697274    |   -0.00814952117861   |\n",
       "|   -0.00985497846215   |    -0.0102424153145   |   -0.000945362361057  |\n",
       "|     0.831173338471    |     0.400522104686    |    0.0087949757352    |\n",
       "|     0.463614351574    |     0.254306309532    |    0.00602985393423   |\n",
       "+-----------------------+-----------------------+-----------------------+\n",
       "[194 rows x 7 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_coefficients_to_table(coefficients_0_penalty, 'coefficients [L2=0]')\n",
    "add_coefficients_to_table(coefficients_4_penalty, 'coefficients [L2=4]')\n",
    "add_coefficients_to_table(coefficients_10_penalty, 'coefficients [L2=10]')\n",
    "add_coefficients_to_table(coefficients_1e2_penalty, 'coefficients [L2=1e2]')\n",
    "add_coefficients_to_table(coefficients_1e3_penalty, 'coefficients [L2=1e3]')\n",
    "add_coefficients_to_table(coefficients_1e5_penalty, 'coefficients [L2=1e5]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the $l2\\_penalty = 10$ and find the 5 most important words (largest coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coefficients [L2=0]', 'coefficients [L2=4]', 'coefficients [L2=10]', 'coefficients [L2=1e2]', 'coefficients [L2=1e3]', 'coefficients [L2=1e5]']\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "coefficients [L2=10]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Five most important words:\n",
      "['loves', 'love', 'easy', 'perfect', 'great']\n"
     ]
    }
   ],
   "source": [
    "my_keys = table.column_names()\n",
    "del my_keys[0]\n",
    "table_temp = table.sort(my_keys[2], ascending = False)\n",
    "print my_keys\n",
    "print '--------------------------------------------------------------------------------------------------------'\n",
    "print my_keys[2]\n",
    "print '--------------------------------------------------------------------------------------------------------'\n",
    "print 'Five most important words:'\n",
    "print table_temp.head(5)['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to check which one of the penalties is the most accurate. We can check the accuracy in the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_accuracy(feature_matrix, sentiment, coefficients):\n",
    "    pred = predict_probability(feature_matrix,coefficients)\n",
    "    predictions = tc.SArray(pred).apply(lambda x: 1 if x > 0.5 else -1)\n",
    "    sentiment = tc.SArray(sentiment)\n",
    "    \n",
    "    num_correct = (predictions == sentiment).sum()\n",
    "    accuracy = 1.0 * num_correct / len(feature_matrix)    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the accuracy for all penalties for the train and vallidation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = {}\n",
    "train_accuracy[0]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_0_penalty)\n",
    "train_accuracy[4]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_4_penalty)\n",
    "train_accuracy[10]  = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_10_penalty)\n",
    "train_accuracy[1e2] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e2_penalty)\n",
    "train_accuracy[1e3] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e3_penalty)\n",
    "train_accuracy[1e5] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e5_penalty)\n",
    "\n",
    "validation_accuracy = {}\n",
    "validation_accuracy[0]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_0_penalty)\n",
    "validation_accuracy[4]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_4_penalty)\n",
    "validation_accuracy[10]  = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_10_penalty)\n",
    "validation_accuracy[1e2] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e2_penalty)\n",
    "validation_accuracy[1e3] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e3_penalty)\n",
    "validation_accuracy[1e5] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e5_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 penalty = 0\n",
      "train accuracy = 0.785586476433, validation_accuracy = 0.785525570862\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 4\n",
      "train accuracy = 0.785539388803, validation_accuracy = 0.785431213436\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 10\n",
      "train accuracy = 0.785633564063, validation_accuracy = 0.785619928288\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100\n",
      "train accuracy = 0.783914865565, validation_accuracy = 0.784204566899\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 1000\n",
      "train accuracy = 0.773673306023, validation_accuracy = 0.771277599547\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100000\n",
      "train accuracy = 0.744267081038, validation_accuracy = 0.740800150972\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(validation_accuracy.keys()):\n",
    "    print \"L2 penalty = %g\" % key\n",
    "    print \"train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[key], validation_accuracy[key])\n",
    "    print \"--------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, the $l2\\_penalty = 10$ got the best accuracy for both train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the Turicreate logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 40402</pre>"
      ],
      "text/plain": [
       "Number of examples          : 40402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 193</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 193</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 194</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-accuracy | Validation-accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-accuracy | Validation-accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 3.005568     | 0.791941          | 0.781853            |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 3.005568     | 0.791941          | 0.781853            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 4.560122     | 0.793228          | 0.780405            |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 4.560122     | 0.793228          | 0.780405            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 6.128424     | 0.793674          | 0.783784            |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 6.128424     | 0.793674          | 0.783784            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 7.682631     | 0.793401          | 0.783784            |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 7.682631     | 0.793401          | 0.783784            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 6        | 9.220086     | 0.793401          | 0.783784            |</pre>"
      ],
      "text/plain": [
       "| 5         | 6        | 9.220086     | 0.793401          | 0.783784            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 7        | 10.741796    | 0.793401          | 0.783784            |</pre>"
      ],
      "text/plain": [
       "| 6         | 7        | 10.741796    | 0.793401          | 0.783784            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modeltc_l2 = tc.logistic_classifier.create(train_data, target='sentiment', features = important_words, \n",
    "                                           l2_penalty = 10, max_iterations = 501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Accuracy of our code:\n",
      "Train accuracy = 0.785633564063, validation_accuracy = 0.785619928288\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy of Turicreate:\n",
      "Train accuracy = 0.792932146725, validation_accuracy = 0.788545008492\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "predictionstc_l2 = modeltc_l2.predict(validation_data, output_type = 'class')\n",
    "resultstc_train_l2 = modeltc_l2.evaluate(train_data)\n",
    "resultstc_val_l2 = modeltc_l2.evaluate(validation_data)\n",
    "print \"--------------------------------------------------------------------------------\"\n",
    "print 'Accuracy of our code:'\n",
    "print \"Train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[10], validation_accuracy[10])\n",
    "print \"--------------------------------------------------------------------------------\"\n",
    "print 'Accuracy of Turicreate:'\n",
    "print \"Train accuracy = %s, validation_accuracy = %s\" % (resultstc_train_l2['accuracy'], resultstc_val_l2['accuracy'])\n",
    "print \"--------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With similar parameters, we got close results. But Turicreate did it in just 6 iterations... we needed 501... )-;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
